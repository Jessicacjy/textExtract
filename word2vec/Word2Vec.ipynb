{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读进所有文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "direc = '/Users/Jesica/Documents/igolden/word2vec/word2vec-tutorial-master/news' # Get current working directory\n",
    "ext = '.txt' # Select your file delimiter\n",
    "\n",
    "file_dict = {} # Create an empty dict\n",
    "txt = ''\n",
    "# Select only files with the ext extension\n",
    "txt_files = [i for i in os.listdir(direc) if os.path.splitext(i)[1] == ext]\n",
    "\n",
    "# Iterate over your txt files\n",
    "for f in txt_files:\n",
    "    # Open them and assign them to file_dict\n",
    "    with open(os.path.join(direc,f)) as file_object:\n",
    "        text = file_object.read()\n",
    "        file_dict[f] = text\n",
    "        txt = txt + '\\n'+ text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_split(str_centence):\n",
    "    list_ret = list()\n",
    "    for s_str in str_centence.split('。'):\n",
    "        if '?' in s_str:\n",
    "            list_ret.extend(s_str.split('？'))\n",
    "        elif '!' in s_str:\n",
    "            list_ret.extend(s_str.split('！'))\n",
    "        else:\n",
    "            list_ret.append(s_str.strip())\n",
    "    return list_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = sentence_split(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39816"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwordset = set()\n",
    "with open('jieba_dict/stopwords.txt','r',encoding = 'utf-8') as sw:\n",
    "    for line in sw:\n",
    "        stopwordset.add(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('word_seg.txt','w')\n",
    "texts_num = 0\n",
    "for line in l:\n",
    "    words = jieba.cut(line, cut_all = False)\n",
    "    for word in words:\n",
    "        if word not in stopwordset:\n",
    "            output.write(word + ' ')\n",
    "    texts_num += 1\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math  \n",
    "from string import punctuation  \n",
    "from heapq import nlargest  \n",
    "from itertools import product, count  \n",
    "from gensim.models import word2vec  \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('word_seg.txt')\n",
    "model = word2vec.Word2Vec(sentences, size =20)\n",
    "model.save('w2v100.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model2 = word2vec.Word2Vec(sentences, size=8, sg=1, hs=1, iter=10)\n",
    "#model2.save('w2v_sg_5.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泛,0.9121560454368591\n",
      "老百姓,0.8621549606323242\n",
      "移动,0.8565236330032349\n",
      "关联性,0.8434779047966003\n",
      "保险公司,0.8427186608314514\n",
      "服务,0.8426828384399414\n",
      "集成,0.8425554633140564\n",
      "支付,0.8418340086936951\n",
      "业务,0.8399962782859802\n",
      "产品,0.8379600048065186\n",
      "弘康,0.834881603717804\n",
      "项目,0.8326399326324463\n",
      "保本,0.8317549228668213\n",
      "开发,0.830572247505188\n",
      "代偿,0.8301917314529419\n",
      "IT,0.8289869427680969\n",
      "代销,0.8243715763092041\n",
      "日渐,0.8197802901268005\n",
      "第三方,0.8194621205329895\n",
      "交银,0.8183554410934448\n"
     ]
    }
   ],
   "source": [
    "res = model.most_similar('保险',topn = 20)\n",
    "for item in res:\n",
    "    print(item[0]+\",\"+str(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_sentences(sentence):  \n",
    "    puns = frozenset(u'。！？')  \n",
    "    tmp = []  \n",
    "    for ch in sentence:  \n",
    "        tmp.append(ch)  \n",
    "        if puns.__contains__(ch):  \n",
    "            yield ''.join(tmp)  \n",
    "            tmp = []  \n",
    "    yield ''.join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_sentences_similarity(sents_1, sents_2):  \n",
    "    ''''' \n",
    "    计算两个句子的相似性 \n",
    "    相同词语的百分比\n",
    "    :param sents_1: \n",
    "    :param sents_2: \n",
    "    :return: \n",
    "    '''  \n",
    "    counter = 0  \n",
    "    for sent in sents_1:  \n",
    "        if sent in sents_2:  \n",
    "            counter += 1  \n",
    "    return counter / (math.log(len(sents_1) + len(sents_2)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph(word_sent):  \n",
    "    \"\"\" \n",
    "    传入句子链表  返回句子之间相似度的图 \n",
    "    :param word_sent: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    num = len(word_sent)  \n",
    "    board = [[0.0 for _ in range(num)] for _ in range(num)]  \n",
    "  \n",
    "    for i, j in product(range(num), repeat=2):  \n",
    "        if i != j:  \n",
    "            \n",
    "            board[i][j] = compute_similarity_by_avg(word_sent[i], word_sent[j])  \n",
    "    return board  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):  \n",
    "    ''''' \n",
    "    计算两个向量之间的余弦相似度 \n",
    "    :param vec1: \n",
    "    :param vec2: \n",
    "    :return: \n",
    "    '''  \n",
    "    tx = np.array(vec1)  \n",
    "    ty = np.array(vec2)  \n",
    "    cos1 = np.sum(tx * ty)  \n",
    "    cos21 = np.sqrt(sum(tx ** 2))  \n",
    "    cos22 = np.sqrt(sum(ty ** 2))  \n",
    "    cosine_value = cos1 / float(cos21 * cos22)  \n",
    "    return cosine_value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity_by_avg(sents_1, sents_2):  \n",
    "    ''''' \n",
    "    对两个句子求平均词向量 \n",
    "    :param sents_1: \n",
    "    :param sents_2: \n",
    "    :return: \n",
    "    '''  \n",
    "    if len(sents_1) == 0 or len(sents_2) == 0:  \n",
    "        return 0.0  \n",
    "    vec1 = model[sents_1[0]]  \n",
    "    for word1 in sents_1[1:]:  \n",
    "        vec1 = vec1 + model[word1]  \n",
    "  \n",
    "    vec2 = model[sents_2[0]]  \n",
    "    for word2 in sents_2[1:]:  \n",
    "        print\n",
    "        vec2 = vec2 + model[word2]  \n",
    "  \n",
    "    similarity = cosine_similarity(vec1 / len(sents_1), vec2 / len(sents_2))  \n",
    "    return similarity  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(weight_graph, scores, i):  \n",
    "    \"\"\" \n",
    "    计算句子在图中的分数 \n",
    "    :param weight_graph: \n",
    "    :param scores: \n",
    "    :param i: \n",
    "    :return: \n",
    "    \"\"\"  \n",
    "    length = len(weight_graph)  \n",
    "    d = 0.85  \n",
    "    added_score = 0.0  \n",
    "  \n",
    "    for j in range(length):  \n",
    "        fraction = 0.0  \n",
    "        denominator = 0.0  \n",
    "        # 计算分子  \n",
    "        fraction = weight_graph[j][i] * scores[j]  \n",
    "        # 计算分母  \n",
    "        for k in range(length):  \n",
    "            denominator += weight_graph[j][k]  \n",
    "            if denominator == 0:  \n",
    "                denominator = 1  \n",
    "        added_score += fraction / denominator  \n",
    "    # 算出最终的分数  \n",
    "    weighted_score = (1 - d) + d * added_score  \n",
    "    return weighted_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_sentences_rank(weight_graph):  \n",
    "    ''''' \n",
    "    输入相似度的图（矩阵) \n",
    "    返回各个句子的分数 \n",
    "    :param weight_graph: \n",
    "    :return: \n",
    "    '''  \n",
    "    # 初始分数设置为0.5  \n",
    "    scores = [0.5 for _ in range(len(weight_graph))]  \n",
    "    old_scores = [0.0 for _ in range(len(weight_graph))]  \n",
    "  \n",
    "    # 开始迭代  \n",
    "    while different(scores, old_scores):  \n",
    "        for i in range(len(weight_graph)):  \n",
    "            old_scores[i] = scores[i]  \n",
    "        for i in range(len(weight_graph)):  \n",
    "            scores[i] = calculate_score(weight_graph, scores, i)  \n",
    "    return scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def different(scores, old_scores):  \n",
    "    ''''' \n",
    "    判断前后分数有无变化 \n",
    "    :param scores: \n",
    "    :param old_scores: \n",
    "    :return: \n",
    "    '''  \n",
    "    flag = False  \n",
    "    for i in range(len(scores)):  \n",
    "        if math.fabs(scores[i] - old_scores[i]) >= 0.0001:  \n",
    "            flag = True  \n",
    "            break  \n",
    "    return flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_symbols(sents):  \n",
    "    stopwords = list(stopwordset)+ ['。', ' ', '.',', ','印']  \n",
    "    _sents = []  \n",
    "    for sentence in sents:  \n",
    "        _sent = []\n",
    "        for word in sentence:  \n",
    "            if word not in stopwordset:  \n",
    "                _sent.append(word)\n",
    "        if _sent:  \n",
    "            _sents.append(_sent)  \n",
    "    return _sents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_model(sents):  \n",
    "    _sents = []  \n",
    "    for sentence in sents:  \n",
    "        _sent = []\n",
    "        for word in sentence:  \n",
    "            if word in model: \n",
    "                _sent.append(word) \n",
    "\n",
    "        if _sent:  \n",
    "            _sents.append(_sent)  \n",
    "    return _sents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, n=0):  \n",
    "    text = text.replace('\\n','')\n",
    "    tokens = cut_sentences(text) \n",
    "    s_count = 0\n",
    "    sentences = []  \n",
    "    sents = []  \n",
    "    ratio = 0.1 #取文本10%\n",
    "    for sent in tokens:  \n",
    "        s_count = s_count + 1\n",
    "        sentences.append(sent)  \n",
    "        sents.append([word for word in jieba.cut(sent) if word ])  \n",
    "#    print(sents)\n",
    "    sents_fs = filter_symbols(sents)  \n",
    "\n",
    "    sents_fm = filter_model(sents_fs)  \n",
    "    graph = create_graph(sents_fm)  \n",
    "    if n==0:\n",
    "        n = math.floor(s_count*ratio)\n",
    "\n",
    "    scores = weight_sentences_rank(graph)  \n",
    "    sent_selected = nlargest(n, zip(scores, count()))  \n",
    "    sent_index = []  \n",
    "\n",
    "    for i in range(n):  \n",
    "        sent_index.append(sent_selected[i][1])  \n",
    "    #return [sentences[i] for i in sent_index]  \n",
    "    return [(sentences[i]) for i in sent_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['书中写道他把每一株豆苗都播种得规规整整，不让它们相互争肥。', '虽然在电影中是被逼无奈，不过在基金定投中我们完全可以通过定投组合来平衡一定这方面的风险。', '基金定投特别适合那些没有大笔钱但是有中长期理财需求的人群，其面临的窘境是够不到高端理财的门槛；轻易投入股市又极有可能成为“新韭菜”；网上各种低风险的XX宝收益日益缩水；然而高风险的P2P又不敢轻易尝试……通过定期定额投资开放式证券基金当中则能够有效地积少成多，因而我们说基金定投是一种比较理想和稳定的大众理财方式。']\n",
      "['书中写道他把每一株豆苗都播种得规规整整，不让它们相互争肥。', '虽然在电影中是被逼无奈，不过在基金定投中我们完全可以通过定投组合来平衡一定这方面的风险。', '基金定投特别适合那些没有大笔钱但是有中长期理财需求的人群，其面临的窘境是够不到高端理财的门槛；轻易投入股市又极有可能成为“新韭菜”；网上各种低风险的XX宝收益日益缩水；然而高风险的P2P又不敢轻易尝试……通过定期定额投资开放式证券基金当中则能够有效地积少成多，因而我们说基金定投是一种比较理想和稳定的大众理财方式。', '我们可以通过一些基金网站上的定投排行榜深度挖掘那些能够始终保持较好水准的优质基金。', '如果一开始不做规划就瞎种，种多少吃多少留多少都不算清楚，显然是活不久的。']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':  \n",
    "    #from IPython.core.debugger import Tracer; Tracer()()\n",
    "    with open(\"test/令人拍手叫好的年度幽默硬科幻巨作《火星救援》当中，主人公马特达蒙饰演的角色在火星上非常努力，种土豆养\"\n",
    "              , \"r\", encoding='utf-8') as myfile:  \n",
    "        text = myfile.read().replace('\\n', '')  \n",
    "        \n",
    "        print(summarize(text)) \n",
    "        print(summarize(text,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
